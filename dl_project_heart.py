# -*- coding: utf-8 -*-
"""DL_Project_heart.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CwiAWtW_SjCR_kmKRel4bX-Ihm3wwl7A
"""

#@title **Deep Learning Project**

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import tensorflow as tf

import warnings
warnings.filterwarnings("ignore")

# To load dataser
df=pd.read_csv("/content/drive/MyDrive/DL case/heart.csv")

df.head()

df.tail()

df.info()

df["target"].value_counts()

df.nunique()

df.isnull().sum()



#@title Data Visualization

df['chol'].hist(color="green")

sns.countplot(df,x="target")

cate_val=[]
cont_val=[]

# creating 2 new object to store
# categorical column(cate_val)
# and other to store numerical column(cont_val)

for column in df.columns:
    if df[column].nunique()<=10:
        cate_val.append(column)
    else:
        cont_val.append(column)

for col in df[cate_val]:
    print(col)
    print(df[col].value_counts())
    
    sns.countplot(df,x=col)
    plt.show()



x=df.drop("target",axis=1)
y=df["target"]

from sklearn.model_selection import train_test_split
xtrain,xtest,ytrain,ytest=train_test_split(x,y,test_size=0.3,random_state=1)
print(xtrain.shape)
print(xtest.shape)
print(ytrain.shape)
print(ytest.shape)

y.value_counts()

xtrain.ndim,type(xtrain)

from sklearn.preprocessing import StandardScaler
ss=StandardScaler()
xtrain=ss.fit_transform(xtrain)
xtest=ss.transform(xtest)

xtrain.ndim,type(xtrain)

x.shape

#create a neural network
#first create a object of Sequential class and pass the no. of layers in list inside the class
model=tf.keras.models.Sequential([
    tf.keras.layers.Dense(units=13,activation='relu', input_shape=(x.shape[1],)), #first hidden layer
    tf.keras.layers.Dense(units=1,activation='sigmoid')  #output layer
])

model.summary()

#compile the model
model.compile(optimizer='sgd',loss='binary_crossentropy',metrics=["accuracy"])



# Create EarlyStopping
from keras.callbacks import EarlyStopping
# create object of earlystopping class
es=EarlyStopping(monitor="val_loss",         #val_loss means testing loss
                 min_delta=0.00001,          #value of lambda
                 patience=20,
                 verbose=1,
                 mode="auto",
                 baseline=None,
                 restore_best_weights=False
)

#Train the model and also check either model is overfit/not overfit
trained_model=model.fit(xtrain,ytrain,epochs=5000,callbacks=es,
                        validation_data=(xtest,ytest))
#trained_model is user defined object

#Evaluating of training data
model.evaluate(xtrain,ytrain)

#evaluation of testing data
model.evaluate(xtest,ytest)

#Visualization training loss and testing loss(val_loss)
plt.plot(trained_model.history["loss"],color="red",label="Training Loss")
plt.plot(trained_model.history["val_loss"],color="blue",label="Testing Loss")
plt.legend()
plt.show()

plt.plot(trained_model.history["accuracy"],color="red",label="Training Accuracy")
plt.plot(trained_model.history["val_accuracy"],color="blue",label="Testing accuracy")
plt.legend()
plt.show()

# To predict or testing the model with 30% data
ypred=model.predict(xtest).round(2)
#formula: yred=1/(1+np.exp(-xtest))
#its return probability's value

ypred

dict={"Actual output":ytest}
#converts dict into DataFrame
df1=pd.DataFrame(dict)
df1.head(10)
# To add new column for ypred
df1["Predicted_output"]=ypred
df1.head(10)

xtest

ytest.ndim

ypred.ndim

ypred=np.where(ypred>=0.5,1,0)
ypred

dict={"Actual output":ytest}
#converts dict into DataFrame
df1=pd.DataFrame(dict)
df1.head(10)
# To add new column for ypred
df1["Predicted_output"]=ypred
df1.head(20)

#generate classification report
from sklearn.metrics import classification_report
print(classification_report(ytest,ypred))

#Confusion Matrix
from sklearn.metrics import confusion_matrix
print(confusion_matrix(ytest,ypred))



model1=tf.keras.models.Sequential([
    tf.keras.layers.Dense(units=13,activation="relu",input_shape=(x.shape[1],)),
    tf.keras.layers.Dense(units=12,activation="relu"),
    tf.keras.layers.Dense(units=1, activation="sigmoid")
])

model1.compile(optimizer="sgd",loss="binary_crossentropy",metrics=["accuracy"])

trained_model1=model1.fit(xtrain,ytrain, callbacks=es, epochs=5000, validation_data=(xtest,ytest))

model1.evaluate(xtrain,ytrain)

model1.evaluate(xtest,ytest)

plt.plot(trained_model1.history["loss"],color="red",label="Training loss")
plt.plot(trained_model1.history["val_loss"],color="blue",label="Testing loss")
plt.legend()
plt.show()

plt.plot(trained_model1.history["accuracy"],color="red",label="Training Accuracy")
plt.plot(trained_model1.history["val_accuracy"],color="blue",label="Testing Accuracy")
plt.legend()
plt.show()

ypred=model1.predict(xtest)
ypred

ypred=np.where(ypred>=0.5,1,0)
ypred

dict={"Actual output":ytest}
#converts dict into DataFrame
df1=pd.DataFrame(dict)
df1.head(10)
# To add new column for ypred
df1["Predicted_output"]=ypred
df1.head(10)

print(classification_report(ytest,ypred))

print(confusion_matrix(ytest,ypred))

